{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of posts we can get per minute is 1200.0\n",
      "The maximum number of posts we can get per day is 1728000.0\n"
     ]
    }
   ],
   "source": [
    "# Max number of requests\n",
    "max_requests = 60/5\n",
    "items_per_request = 100\n",
    "max_posts = max_requests * items_per_request\n",
    "print(\"The maximum number of posts we can get per minute is\", max_posts)\n",
    "print(\"The maximum number of posts we can get per day is\", max_posts * 24 * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_reddit(query, limit):\n",
    "    base_url = 'https://www.reddit.com/search.json'\n",
    "    headers = {'User-agent': 'yourbot'}\n",
    "    posts = []\n",
    "    after = None\n",
    "\n",
    "    while len(posts) < limit:\n",
    "        params = {\n",
    "        'q': query,\n",
    "        'limit': 100,  # Reddit API returns max 100 posts per request\n",
    "        'after': after,\n",
    "        'sort': sort,\n",
    "        't': t\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(base_url, headers=headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            new_posts = data['data']['children']\n",
    "            if not new_posts:\n",
    "                print('No more posts found.')\n",
    "                break\n",
    "            posts.extend(new_posts)\n",
    "            \n",
    "            after = data['data']['after']\n",
    "            print(\"after:\", after)\n",
    "            if after is None:\n",
    "                print('No more posts to fetch.')\n",
    "                break\n",
    "                \n",
    "            \n",
    "            # Print progress\n",
    "            print(f'Retrieved {len(posts)} posts so far...')\n",
    "            \n",
    "            # Sleep to avoid hitting rate limits\n",
    "            time.sleep(5)\n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            print(f'HTTP error occurred: {err}')\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f'An Error Occurred: {e}')\n",
    "            break\n",
    "\n",
    "    return posts[:limit]\n",
    "\n",
    "def fetch_comments(post_permalink):\n",
    "    base_url = f'https://www.reddit.com{post_permalink}.json'\n",
    "    headers = {'User-agent': 'yourbot'}\n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        comments = []\n",
    "        for comment in data[1]['data']['children']:\n",
    "            if comment['kind'] == 't1':  # Ensure it's a comment\n",
    "                comments.append(comment['data']['body'])\n",
    "        return comments\n",
    "        time.sleep(5)\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(f'HTTP error occurred: {err}')\n",
    "    except Exception as e:\n",
    "        print(f'An Error Occurred: {e}')\n",
    "    return []\n",
    "\n",
    "def convert_timestamp_to_date(posts):\n",
    "    for post in posts:\n",
    "        timestamp = post['data']['created_utc']\n",
    "        date = datetime.fromtimestamp(timestamp, tz=timezone.utc).strftime('%Y/%m/%d')\n",
    "        post['data']['created_date'] = date\n",
    "    return posts\n",
    "\n",
    "# export to Excel\n",
    "def get_post_data(posts, comments=False):\n",
    "    data = []\n",
    "    for post in posts:\n",
    "        x = post['data']\n",
    "        if comments:\n",
    "            comments = fetch_comments(x['permalink'])\n",
    "            print(x[\"author_fullname\"],len(comments))\n",
    "            data.append([x['title'], x['selftext'], x.get(\"author_fullname\", \"N/A\"),\n",
    "                         x['url'], x['created_date'], x['num_comments'], x['score'], x['subreddit_name_prefixed'], comments])\n",
    "        else:\n",
    "            data.append([x['title'], x['selftext'], x[\"author_fullname\"],\n",
    "                         x['url'], x['created_date'], x['num_comments'], x['score'], x['subreddit_name_prefixed']])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = [\"careerguidance\"]\n",
    "# subreddits = [\"careerguidance\", \"jobs\", \"antiwork\", \"Economics\",\n",
    "#               \"MachineLearning\", \"ChatGPT\", \"technology\", \"artificial\", \"OpenAI\",\n",
    "#               \"changemyview\", \"AskReddit\", \"Futurology\", \"Showerthoughts\", \"NoStupidQuestions\"]\n",
    "\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    query = f'(ai OR \"artificial intelligence\" OR chatgpt) AND (job OR jobs OR work OR career OR employment OR profession) AND (replace OR replaced OR replaces OR replacement OR affected OR affect OR affecting OR disappear OR disappearing OR disappeared OR fired OR hiring OR hire OR lose OR lost OR losing OR eliminate OR eliminates OR eliminating OR redundant OR safe OR obsolete OR threaten) subreddit:{subreddit}'#Max 500 characters \n",
    "    \n",
    "    limit = 1000\n",
    "    sort = 'top'  # 'relevance', 'hot', 'top', 'new', 'comments'\n",
    "    t = 'all'  # all, day, hour, month, week, year\n",
    "\n",
    "    posts = search_reddit(query, limit)\n",
    "    posts = convert_timestamp_to_date(posts)\n",
    "\n",
    "    print(f\"Retrieved {len(posts)} posts\")\n",
    "\n",
    "    if len(posts) != 0:\n",
    "        data = get_post_data(posts, comments=True)\n",
    "        # export_query = query.replace(\":\", \"_\").replace('\"', '-') + f\" sort_{sort} t_{t}\"\n",
    "        export_query = f\"long_query subreddit_{subreddit} sort_{sort} t_{t} n_{len(posts)}\"\n",
    "        pd.DataFrame(data).to_excel(f'../data/{export_query}_with_comments.xlsx', header=['Title', 'Body', 'Author', 'URL', 'Created', 'Comments', 'Score', 'Subreddit'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use .py instead"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
